{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy that first appeared in Richard Bandler and John\n",
    "Grinder's book The Structure of Magic I (1975). NLP asserts a connection between neurological processes, language, and acquired behavioral patterns, and that these can \n",
    "be changed to achieve specific goals in life.[1][2] According to Bandler and Grinder, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic \n",
    "illnesses, near-sightedness,[a] allergy, the common cold,[a] and learning disorders,[3][4] often in a single session. They also say that NLP can model the skills of \n",
    "exceptional people, allowing anyone to acquire them.[5][b]\n",
    "NLP has been adopted by some hypnotherapists as well as by companies that run seminars marketed as leadership training to businesses and government agencies.[6][7]\n",
    "No scientific evidence supports the claims made by NLP advocates, and it has been called a pseudoscience.[8][9][10] Scientific reviews have shown that NLP is based on o\n",
    "utdated metaphors of the brain's inner workings that are inconsistent with current neurological theory, and that NLP contains numerous factual errors.[7][11] Reviews also \n",
    "found that research that favored NLP contained significant methodological flaws, and that three times as many studies of a much higher quality failed to reproduce the claims \n",
    "made by Bandler, Grinder, and other NLP practitioners.[9][10].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18de8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install scipy\n",
    "#pip install nltk\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc3761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import scipy\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0ceed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d43254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "print(ps.stem('histories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa7ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n"
     ]
    }
   ],
   "source": [
    "wl= WordNetLemmatizer()\n",
    "print(wl.lemmatize('histories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e03a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Tokecize:Convert the text to sentences then to words:\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110ab77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wl.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6a2ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neuro linguistic programming nlp pseudoscientific approach communication personal development psychotherapy first appeared richard bandler john grinder book structure magic',\n",
       " 'nlp asserts connection neurological process language acquired behavioral pattern changed achieve specific goal life',\n",
       " 'according bandler grinder nlp treat problem phobia depression tic disorder psychosomatic illness near sightedness allergy common cold learning disorder often single session',\n",
       " 'also say nlp model skill exceptional people allowing anyone acquire',\n",
       " 'b nlp adopted hypnotherapists well company run seminar marketed leadership training business government agency',\n",
       " 'scientific evidence support claim made nlp advocate called pseudoscience',\n",
       " 'scientific review shown nlp based utdated metaphor brain inner working inconsistent current neurological theory nlp contains numerous factual error',\n",
       " 'review also found research favored nlp contained significant methodological flaw three time many study much higher quality failed reproduce claim made bandler grinder nlp practitioner',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17699fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59fe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bag of  word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bw = CountVectorizer(ngram_range=(1,1))\n",
    "X= bw.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38534cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neuro': 66,\n",
       " 'linguistic': 56,\n",
       " 'programming': 78,\n",
       " 'nlp': 68,\n",
       " 'pseudoscientific': 80,\n",
       " 'approach': 12,\n",
       " 'communication': 25,\n",
       " 'personal': 73,\n",
       " 'development': 32,\n",
       " 'psychotherapy': 82,\n",
       " 'first': 40,\n",
       " 'appeared': 11,\n",
       " 'richard': 87,\n",
       " 'bandler': 14,\n",
       " 'john': 51,\n",
       " 'grinder': 45,\n",
       " 'book': 17,\n",
       " 'structure': 99,\n",
       " 'magic': 58,\n",
       " 'asserts': 13,\n",
       " 'connection': 27,\n",
       " 'neurological': 67,\n",
       " 'process': 77,\n",
       " 'language': 52,\n",
       " 'acquired': 3,\n",
       " 'behavioral': 16,\n",
       " 'pattern': 71,\n",
       " 'changed': 21,\n",
       " 'achieve': 1,\n",
       " 'specific': 98,\n",
       " 'goal': 43,\n",
       " 'life': 55,\n",
       " 'according': 0,\n",
       " 'treat': 107,\n",
       " 'problem': 76,\n",
       " 'phobia': 74,\n",
       " 'depression': 31,\n",
       " 'tic': 104,\n",
       " 'disorder': 33,\n",
       " 'psychosomatic': 81,\n",
       " 'illness': 48,\n",
       " 'near': 65,\n",
       " 'sightedness': 94,\n",
       " 'allergy': 7,\n",
       " 'common': 24,\n",
       " 'cold': 23,\n",
       " 'learning': 54,\n",
       " 'often': 70,\n",
       " 'single': 96,\n",
       " 'session': 92,\n",
       " 'also': 9,\n",
       " 'say': 89,\n",
       " 'model': 63,\n",
       " 'skill': 97,\n",
       " 'exceptional': 36,\n",
       " 'people': 72,\n",
       " 'allowing': 8,\n",
       " 'anyone': 10,\n",
       " 'acquire': 2,\n",
       " 'adopted': 4,\n",
       " 'hypnotherapists': 47,\n",
       " 'well': 109,\n",
       " 'company': 26,\n",
       " 'run': 88,\n",
       " 'seminar': 91,\n",
       " 'marketed': 60,\n",
       " 'leadership': 53,\n",
       " 'training': 106,\n",
       " 'business': 19,\n",
       " 'government': 44,\n",
       " 'agency': 6,\n",
       " 'scientific': 90,\n",
       " 'evidence': 35,\n",
       " 'support': 101,\n",
       " 'claim': 22,\n",
       " 'made': 57,\n",
       " 'advocate': 5,\n",
       " 'called': 20,\n",
       " 'pseudoscience': 79,\n",
       " 'review': 86,\n",
       " 'shown': 93,\n",
       " 'based': 15,\n",
       " 'utdated': 108,\n",
       " 'metaphor': 61,\n",
       " 'brain': 18,\n",
       " 'inner': 50,\n",
       " 'working': 110,\n",
       " 'inconsistent': 49,\n",
       " 'current': 30,\n",
       " 'theory': 102,\n",
       " 'contains': 29,\n",
       " 'numerous': 69,\n",
       " 'factual': 37,\n",
       " 'error': 34,\n",
       " 'found': 42,\n",
       " 'research': 85,\n",
       " 'favored': 39,\n",
       " 'contained': 28,\n",
       " 'significant': 95,\n",
       " 'methodological': 62,\n",
       " 'flaw': 41,\n",
       " 'three': 103,\n",
       " 'time': 105,\n",
       " 'many': 59,\n",
       " 'study': 100,\n",
       " 'much': 64,\n",
       " 'higher': 46,\n",
       " 'quality': 83,\n",
       " 'failed': 38,\n",
       " 'reproduce': 84,\n",
       " 'practitioner': 75}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw.vocabulary_\n",
    "# This means \"neuro\" is column 66, \"linguistic\" is column 56, \"programming\" is column 78, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e44883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neuro linguistic programming nlp pseudoscientific approach communication personal development psychotherapy first appeared richard bandler john grinder book structure magic'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "358ac464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41eea74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(ngram_range = (1,1))\n",
    "XX= tf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8dd75ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24071566, 0.24071566, 0.        , 0.17677416,\n",
       "        0.        , 0.        , 0.24071566, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24071566, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24071566, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24071566, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17677416, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24071566, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24071566, 0.        , 0.24071566, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24071566, 0.        , 0.10196739, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24071566, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24071566, 0.        ,\n",
       "        0.24071566, 0.        , 0.24071566, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24071566, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24071566,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773b1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d669c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "426a1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "094c1429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP in vocab? Yes\n",
      "psychotherapy in vocab? Yes\n",
      "communication in vocab? Yes\n",
      "science in vocab? Yes\n",
      "language in vocab? Yes\n"
     ]
    }
   ],
   "source": [
    "for word in ['NLP', 'psychotherapy', 'communication', 'science', 'language']:\n",
    "    print(f\"{word} in vocab? {'Yes' if word in wv else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d9679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.71289062e-02,  2.27539062e-01,  9.47265625e-02,  1.25976562e-01,\n",
       "        4.71191406e-02, -9.27734375e-02,  5.41992188e-02,  2.43164062e-01,\n",
       "        1.07910156e-01, -2.75390625e-01,  1.65039062e-01, -2.14843750e-01,\n",
       "        3.49121094e-02,  2.98828125e-01, -1.53198242e-02,  2.19726562e-01,\n",
       "        1.04492188e-01,  4.23828125e-01,  2.99072266e-02, -3.69140625e-01,\n",
       "       -1.36718750e-01, -2.75390625e-01, -3.63281250e-01, -4.00390625e-02,\n",
       "       -1.29882812e-01,  2.89306641e-02, -2.67578125e-01,  2.85156250e-01,\n",
       "       -2.91015625e-01, -2.03125000e-01, -1.05468750e-01, -1.96533203e-02,\n",
       "       -9.08203125e-02, -2.09960938e-02,  2.29492188e-02,  4.53125000e-01,\n",
       "        1.31835938e-01, -1.28906250e-01, -2.16484070e-04,  1.27929688e-01,\n",
       "        8.83789062e-02,  1.61132812e-01, -1.04003906e-01,  1.41601562e-01,\n",
       "        3.76953125e-01, -3.88183594e-02, -2.96630859e-02,  8.20312500e-02,\n",
       "        3.14941406e-02, -3.86718750e-01, -8.69140625e-02, -9.42382812e-02,\n",
       "       -2.38281250e-01, -5.24902344e-02, -1.51367188e-01,  7.22656250e-02,\n",
       "       -2.20703125e-01, -4.00390625e-02, -9.13085938e-02, -3.75000000e-01,\n",
       "        1.53320312e-01,  5.71289062e-02,  6.54296875e-02,  3.36914062e-02,\n",
       "       -2.39257812e-01,  1.00097656e-01, -1.67968750e-01,  5.27343750e-02,\n",
       "       -2.21679688e-01, -4.14062500e-01,  1.46484375e-01,  1.03027344e-01,\n",
       "        1.05468750e-01, -1.27929688e-01,  9.52148438e-03,  6.29882812e-02,\n",
       "        6.25000000e-02,  4.32128906e-02,  3.14453125e-01,  6.25000000e-02,\n",
       "       -9.17968750e-02, -2.54821777e-03, -1.20239258e-02, -1.45507812e-01,\n",
       "        2.13867188e-01, -6.29882812e-02, -5.23437500e-01,  7.56835938e-02,\n",
       "       -6.05468750e-02, -1.92871094e-02,  2.52685547e-02, -1.35742188e-01,\n",
       "        9.71679688e-02,  1.27929688e-01, -5.10253906e-02, -2.83203125e-02,\n",
       "       -2.25830078e-02,  1.53320312e-01,  1.81640625e-01, -1.66015625e-01,\n",
       "        1.02539062e-02, -1.69921875e-01, -1.47460938e-01,  1.56250000e-01,\n",
       "        2.35351562e-01, -1.56250000e-02, -1.09863281e-01,  6.03027344e-02,\n",
       "        2.27539062e-01,  2.19726562e-01,  1.57226562e-01,  2.47070312e-01,\n",
       "        1.72851562e-01, -3.76953125e-01,  8.83789062e-02,  9.42382812e-02,\n",
       "       -9.27734375e-02, -9.96093750e-02,  1.26953125e-01,  2.50000000e-01,\n",
       "       -3.45703125e-01, -1.78222656e-02,  4.66796875e-01,  7.56835938e-02,\n",
       "        1.51367188e-01,  1.53320312e-01, -3.30078125e-01,  1.01470947e-03,\n",
       "       -1.65039062e-01,  1.66992188e-01, -1.45507812e-01, -4.19921875e-01,\n",
       "        6.59179688e-02,  1.41143799e-03, -1.47460938e-01,  1.24511719e-02,\n",
       "       -5.00488281e-02, -2.94921875e-01, -1.55273438e-01,  2.36816406e-02,\n",
       "       -4.83398438e-02, -3.27148438e-02,  4.37011719e-02,  3.22265625e-01,\n",
       "        8.78906250e-02, -1.38671875e-01,  1.75781250e-01, -1.04492188e-01,\n",
       "       -2.37304688e-01,  1.08398438e-01, -7.81250000e-02,  7.47070312e-02,\n",
       "       -3.24218750e-01, -1.38671875e-01,  6.93359375e-02, -1.87500000e-01,\n",
       "       -2.43164062e-01,  1.26953125e-01, -3.00292969e-02, -2.67578125e-01,\n",
       "        3.66210938e-02, -1.28906250e-01, -2.08984375e-01, -1.44531250e-01,\n",
       "        1.32812500e-01,  1.66992188e-01,  1.18164062e-01, -1.51367188e-01,\n",
       "       -2.59765625e-01, -1.85546875e-02, -5.51757812e-02, -1.68945312e-01,\n",
       "        6.43920898e-03, -2.50000000e-01,  3.10546875e-01, -3.12500000e-01,\n",
       "       -1.34765625e-01, -5.88378906e-02, -1.00585938e-01, -5.90820312e-02,\n",
       "       -9.08203125e-02, -4.76074219e-02, -6.29882812e-02, -1.27929688e-01,\n",
       "       -1.01074219e-01, -9.17968750e-02, -2.69531250e-01,  7.51953125e-02,\n",
       "        8.59375000e-02,  1.96289062e-01,  1.59179688e-01, -2.51953125e-01,\n",
       "        1.05468750e-01, -2.17773438e-01, -1.91650391e-02, -5.54199219e-02,\n",
       "       -7.78198242e-03,  1.13525391e-02, -9.17968750e-02, -1.03027344e-01,\n",
       "        2.27539062e-01,  2.31445312e-01,  1.33789062e-01, -2.06298828e-02,\n",
       "        1.04003906e-01, -2.84423828e-02, -8.10546875e-02, -2.47070312e-01,\n",
       "       -5.39550781e-02, -1.68457031e-02,  5.88378906e-02,  6.49414062e-02,\n",
       "       -1.23046875e-01,  3.12500000e-01,  1.63085938e-01,  1.23046875e-01,\n",
       "        1.46484375e-01,  3.33984375e-01,  5.24902344e-02, -2.44140625e-01,\n",
       "       -1.29882812e-01,  2.91748047e-02, -2.85156250e-01, -8.34960938e-02,\n",
       "       -6.20117188e-02, -1.66992188e-01,  1.36108398e-02,  4.44335938e-02,\n",
       "       -2.21679688e-01, -8.30078125e-03,  6.78710938e-02, -1.57226562e-01,\n",
       "       -9.09423828e-03,  5.10253906e-02, -3.14941406e-02, -1.99218750e-01,\n",
       "        1.15722656e-01,  6.29882812e-02, -5.10253906e-02, -2.42187500e-01,\n",
       "        1.67968750e-01, -3.47656250e-01, -8.49609375e-02,  1.03027344e-01,\n",
       "       -1.08398438e-01,  3.14453125e-01, -1.60156250e-01,  4.12109375e-01,\n",
       "       -6.44531250e-02,  9.57031250e-02, -1.92382812e-01,  3.49121094e-02,\n",
       "        8.98437500e-02, -2.03125000e-01, -2.00195312e-01, -2.75390625e-01,\n",
       "        1.18164062e-01, -7.22656250e-02, -3.00598145e-03,  1.15722656e-01,\n",
       "        1.47460938e-01,  2.18505859e-02, -2.55859375e-01,  2.59765625e-01,\n",
       "       -5.83496094e-02,  2.53906250e-01, -1.69921875e-01,  1.04980469e-01,\n",
       "       -2.42919922e-02, -2.20703125e-01, -1.54418945e-02,  1.87500000e-01,\n",
       "        8.25195312e-02,  4.17968750e-01,  2.43164062e-01, -2.51953125e-01,\n",
       "        4.71191406e-02, -2.17285156e-02, -1.34765625e-01, -1.72851562e-01,\n",
       "        2.19726562e-01, -1.70898438e-01,  7.71484375e-02, -5.12695312e-02,\n",
       "       -2.57812500e-01, -2.05078125e-01,  2.92968750e-02,  2.26562500e-01,\n",
       "       -3.46679688e-02, -1.32446289e-02, -1.17675781e-01,  1.82617188e-01,\n",
       "       -1.19628906e-02,  2.00195312e-01, -2.27539062e-01, -9.81445312e-02,\n",
       "        1.63085938e-01, -3.12500000e-01,  2.29492188e-01,  1.05468750e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['histories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaa54f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138046622276306),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422104597091675)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0575ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"man\",\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "548354b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own Word2Vec\n",
    "sentences_for_word2vec = [sentence.split() for sentence in corpus if sentence]  # remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "265baa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=sentences_for_word2vec,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0872a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('contained', 0.29159700870513916), ('run', 0.23731066286563873), ('life', 0.22668011486530304), ('neurological', 0.21837154030799866), ('contains', 0.20869553089141846), ('session', 0.1981584131717682), ('acquired', 0.17630547285079956), ('appeared', 0.17210491001605988), ('goal', 0.15857431292533875), ('exceptional', 0.14514818787574768)]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Explore\n",
    "print(model.wv.most_similar('nlp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed157e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
